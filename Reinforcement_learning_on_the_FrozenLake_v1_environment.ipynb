{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wm9BZxQJnHcS",
    "outputId": "d6c28c54-11d4-454c-eff9-a9e84d9bc310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/626.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.2/626.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install gym==0.23.1 --quiet\n",
    "!pip install tensorflow --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "doq5CMXVm30y",
    "outputId": "272503ff-37f6-47e8-b1b3-d49e19cdc5d1"
   },
   "outputs": [],
   "source": [
    "#Library for environments\n",
    "import gym\n",
    "from gym.envs.registration import register\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "#Librairies to represent the output\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Essential libraries for computation\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LVWg0ts2nrtR"
   },
   "outputs": [],
   "source": [
    "random_map = generate_random_map(size=10, p=0.3)\n",
    "env = gym.make(\"FrozenLake-v1\", desc=random_map)\n",
    "env.reset()\n",
    "plt.imshow(env.render('rgb-array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOXzimOcsSfl",
    "outputId": "ca012a29-de8b-4caf-ba52-23c1cc86a8bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Get GPU device name\n",
    "if tf.test.gpu_device_name():\n",
    "    print('\\nDefault GPU Device:', tf.test.gpu_device_name())\n",
    "else:\n",
    "    print('\\nNo GPU device found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tNTIESGmtLJo"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Deep Q-Network agent for the FrozenLake environment.\n",
    "\n",
    "    The agent uses a neural network to approximate the Q-function and\n",
    "    implements epsilon-greedy exploration strategy with decay.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Set device before other initializations\n",
    "        self.device_name = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device_name}\")\n",
    "\n",
    "        # Environment parameters\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Learning parameters\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        self.epsilon = 1.0  # Initial exploration rate\n",
    "        self.epsilon_min = 0.1  # Minimum exploration rate\n",
    "        self.epsilon_decay = 0.995  # Decay rate for exploration\n",
    "        self.learning_rate = 0.001  # Learning rate for the optimizer\n",
    "\n",
    "        # Memory for experience replay\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.batch_size = 32\n",
    "\n",
    "        # Create the neural network model\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()  # Initialize target model to be same as model\n",
    "\n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Neural Network to approximate Q-value function:\n",
    "        * Input: state\n",
    "        * Output: Q-values for each action\n",
    "        \"\"\"\n",
    "        model = nn.Sequential(\n",
    "            nn.Linear(self.state_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, self.action_size)\n",
    "        ).to(self.device_name)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"Update the target model with the weights of the model\"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in memory for replay\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Choose action using epsilon-greedy policy\n",
    "        \"\"\"\n",
    "        if random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device_name)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.model(state)\n",
    "        return torch.argmax(q_values[0]).item()\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"\n",
    "        Train the network using experience replay\n",
    "        \"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Sample random experiences from memory\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        states = torch.FloatTensor([x[0] for x in minibatch]).to(self.device_name)\n",
    "        next_states = torch.FloatTensor([x[3] for x in minibatch]).to(self.device_name)\n",
    "        actions = torch.LongTensor([x[1] for x in minibatch]).to(self.device_name)\n",
    "        rewards = torch.FloatTensor([x[2] for x in minibatch]).to(self.device_name)\n",
    "        dones = torch.BoolTensor([x[4] for x in minibatch]).to(self.device_name)\n",
    "\n",
    "        # Predict Q-values for current and next states\n",
    "        current_q_values = self.model(states)\n",
    "        next_q_values = self.target_model(next_states)\n",
    "\n",
    "        # Create training targets\n",
    "        targets = current_q_values.clone()\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                targets[i, actions[i]] = rewards[i]\n",
    "            else:\n",
    "                targets[i, actions[i]] = rewards[i] + self.gamma * torch.max(next_q_values[i])\n",
    "\n",
    "        # Compute loss\n",
    "        loss = nn.MSELoss()(current_q_values, targets)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def convert_state(self, state):\n",
    "        \"\"\"Convert state number to one-hot encoding\"\"\"\n",
    "        state_one_hot = np.zeros(self.state_size)\n",
    "        state_one_hot[state] = 1\n",
    "        return state_one_hot\n",
    "\n",
    "# Training loop helper function\n",
    "def train_dqn_agent(env, agent, episodes=1000):\n",
    "    \"\"\"\n",
    "    Train the DQN agent on the environment\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for episode in range(episodes):\n",
    "        # Reset environment\n",
    "        state, _ = env.reset()\n",
    "        state = agent.convert_state(state)\n",
    "        score = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Choose and take action\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            # Convert next_state to one-hot and store experience\n",
    "            next_state = agent.convert_state(next_state)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            # Train the network\n",
    "            agent.replay()\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "        # Print progress\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_score = np.mean(scores[-100:])\n",
    "            print(f\"Episode: {episode + 1}, Average Score: {avg_score:.2f}, Epsilon: {agent.epsilon:.2f}\")\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Helper function to create and initialize the agent\n",
    "def create_agent(env):\n",
    "    \"\"\"\n",
    "    Create and initialize a DQN agent for the given environment\n",
    "    \"\"\"\n",
    "    state_size = env.observation_space.n  # Number of states\n",
    "    action_size = env.action_space.n      # Number of actions\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "id": "-znojknkuQtb",
    "outputId": "1778f74c-c5c2-4d92-8203-f6d835a92c0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize the environment and agent\n",
    "random_map = generate_random_map(size=4, p=0.1)\n",
    "env = gym.make(\"FrozenLake-v1\", desc=random_map, is_slippery=True)\n",
    "agent = create_agent(env)\n",
    "\n",
    "# Training parameters\n",
    "n_episodes = 1000\n",
    "max_steps = 100  # Maximum steps per episode\n",
    "training_history = {\n",
    "    'scores': [],\n",
    "    'avg_scores': [],\n",
    "    'epsilons': [],\n",
    "    'steps': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "e5ZWvrQ0u_EX",
    "outputId": "71e6cba7-6db2-4185-f54d-36191c321185"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mztan\\AppData\\Local\\Temp\\ipykernel_29600\\3991647148.py:82: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  states = torch.FloatTensor([x[0] for x in minibatch]).to(self.device_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.00\n",
      "Epsilon: 0.363\n",
      "Steps: 13\n",
      "----------------------------------------\n",
      "Episode: 100/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.01\n",
      "Epsilon: 0.100\n",
      "Steps: 1\n",
      "----------------------------------------\n",
      "Episode: 150/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.02\n",
      "Epsilon: 0.100\n",
      "Steps: 74\n",
      "----------------------------------------\n",
      "Episode: 200/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.04\n",
      "Epsilon: 0.100\n",
      "Steps: 12\n",
      "----------------------------------------\n",
      "Episode: 250/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.05\n",
      "Epsilon: 0.100\n",
      "Steps: 6\n",
      "----------------------------------------\n",
      "Episode: 300/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.04\n",
      "Epsilon: 0.100\n",
      "Steps: 7\n",
      "----------------------------------------\n",
      "Episode: 350/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.04\n",
      "Epsilon: 0.100\n",
      "Steps: 6\n",
      "----------------------------------------\n",
      "Episode: 400/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.03\n",
      "Epsilon: 0.100\n",
      "Steps: 15\n",
      "----------------------------------------\n",
      "Episode: 450/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.06\n",
      "Epsilon: 0.100\n",
      "Steps: 1\n",
      "----------------------------------------\n",
      "Episode: 500/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.05\n",
      "Epsilon: 0.100\n",
      "Steps: 2\n",
      "----------------------------------------\n",
      "Episode: 550/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.00\n",
      "Epsilon: 0.100\n",
      "Steps: 11\n",
      "----------------------------------------\n",
      "Episode: 600/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.03\n",
      "Epsilon: 0.100\n",
      "Steps: 13\n",
      "----------------------------------------\n",
      "Episode: 650/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.06\n",
      "Epsilon: 0.100\n",
      "Steps: 24\n",
      "----------------------------------------\n",
      "Episode: 700/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.05\n",
      "Epsilon: 0.100\n",
      "Steps: 27\n",
      "----------------------------------------\n",
      "Episode: 750/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.02\n",
      "Epsilon: 0.100\n",
      "Steps: 17\n",
      "----------------------------------------\n",
      "Episode: 800/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.02\n",
      "Epsilon: 0.100\n",
      "Steps: 11\n",
      "----------------------------------------\n",
      "Episode: 850/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.02\n",
      "Epsilon: 0.100\n",
      "Steps: 5\n",
      "----------------------------------------\n",
      "Episode: 900/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.02\n",
      "Epsilon: 0.100\n",
      "Steps: 5\n",
      "----------------------------------------\n",
      "Episode: 950/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.03\n",
      "Epsilon: 0.100\n",
      "Steps: 10\n",
      "----------------------------------------\n",
      "Episode: 1000/1000\n",
      "Score: 0.00\n",
      "Average Score (last 100): 0.04\n",
      "Epsilon: 0.100\n",
      "Steps: 3\n",
      "----------------------------------------\n",
      "\n",
      "Training completed!\n",
      "Final average score: 0.04\n",
      "Final epsilon: 0.100\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()  # Just get the state\n",
    "    state = agent.convert_state(state)\n",
    "    score = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Choose and take action\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)  # Updated step unpacking\n",
    "\n",
    "        # Convert next_state and store experience\n",
    "        next_state = agent.convert_state(next_state)\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "        # Move to next state\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        steps += 1\n",
    "\n",
    "        # Train the network\n",
    "        agent.replay()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Store training history\n",
    "    training_history['scores'].append(score)\n",
    "    training_history['epsilons'].append(agent.epsilon)\n",
    "    training_history['steps'].append(steps)\n",
    "\n",
    "    # Calculate average score over last 100 episodes\n",
    "    avg_score = np.mean(training_history['scores'][-100:])\n",
    "    training_history['avg_scores'].append(avg_score)\n",
    "\n",
    "    # Print progress\n",
    "    if (episode + 1) % 50 == 0:\n",
    "        print(f\"Episode: {episode + 1}/{n_episodes}\")\n",
    "        print(f\"Score: {score:.2f}\")\n",
    "        print(f\"Average Score (last 100): {avg_score:.2f}\")\n",
    "        print(f\"Epsilon: {agent.epsilon:.3f}\")\n",
    "        print(f\"Steps: {steps}\")\n",
    "        print(\"-\" * 40)\n",
    "        # Optional: render the environment to see the agent's behavior\n",
    "        if episode % 100 == 0:\n",
    "            env.render()\n",
    "\n",
    "# Print final training results\n",
    "print(\"\\nTraining completed!\")\n",
    "print(f\"Final average score: {training_history['avg_scores'][-1]:.2f}\")\n",
    "print(f\"Final epsilon: {agent.epsilon:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aima",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
